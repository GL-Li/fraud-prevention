{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks and methods:\n",
    "- We will identify ~400 high risk transactions each month for the team to\n",
    "  review.\n",
    "- The model will predict the probability of a transaction being fraud. \n",
    "  - Use logistic regression as it produces well-defined probability.\n",
    "  - Use 2017 data as training data and 2018 Jan data as test data\n",
    "- Metrics: as much fraud loss prevented as possible. \n",
    "  - For this purpose we will use fraud_probability * transactionAmount to rank\n",
    "    each transaction.\n",
    "  - Pick the top 400 transaction for review.\n",
    "\n",
    "## Retults:\n",
    "- We set January 2018 as the test data, which has 39 frauds reported with a\n",
    "  total loss of 5814.46 GBP.\n",
    "- Using the model and team review, we identified 10 frauds and prevented loss\n",
    "  of 4079.30 GBP.\n",
    "- We prevented 70% fraud loss in terms of value.\n",
    "\n",
    "## Model training\n",
    "### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.metrics import recall_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_levels(data, features, lab_data=None, n=20):\n",
    "    \"\"\"\n",
    "    For categorical features, keep levels that have at least n fraud reported\n",
    "    and all other levels groupd as \"others\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: data frame whose categorical features are processed\n",
    "    features: string, list of selected features to be process\n",
    "    lab_data: labeled transaction data to generate fraud count of each levels\n",
    "    n: integer, threshold of reported fraud cases. \n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    data frame\n",
    "    \"\"\"\n",
    "    if lab_data is None:\n",
    "        lab_data = data\n",
    "    \n",
    "    for ft in features:\n",
    "        total_fraud = lab_data.groupby(ft).fraud.sum()\n",
    "        level_keep = total_fraud.index[total_fraud >= n].tolist()\n",
    "        data.loc[~data[ft].isin(level_keep), ft] = 'others'\n",
    "        \n",
    "    return data\n",
    "\n",
    "def clean_trans(data):\n",
    "    \"\"\"\n",
    "    To clean transaction data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: data frame, in which 'transactionTime' was set as index\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    data frame\n",
    "    \"\"\"\n",
    "    data = change_types(data)\n",
    "    data = bin_trans_amount(data)\n",
    "    data = bin_cash(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def change_types(data):\n",
    "    \"\"\"\n",
    "    To change data types of features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: data frame, in which 'transactionTime' was set as index\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    data frame\n",
    "    \"\"\"\n",
    "    \n",
    "    trans['month'] = trans.index.month.astype(str)\n",
    "    trans['weekday'] = trans.index.weekday.astype(str)\n",
    "    trans['hour'] = trans.index.hour.astype(str)\n",
    "\n",
    "    # convert integer to string\n",
    "    for ft in [\"mcc\", \"merchantCountry\", \"posEntryMode\"]:\n",
    "        trans[ft] = trans[ft].astype(str)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def bin_trans_amount(data, all_data=None, n=10):\n",
    "    \"\"\"\n",
    "    To cut 'transactionAmount' into n bins with roughly equal size.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: data frame, transaction data in which transactionAmount is binned\n",
    "    all_data: data frame, all transactions used to find bins\n",
    "    n: integer, number of bins\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    data frame of binned data\n",
    "    \"\"\"\n",
    "    if all_data is None:\n",
    "        all_data = data.copy()\n",
    "    \n",
    "    # use pd.qcut to first find the bins with equal number of samples and then\n",
    "    # set ends to infinity to allow new transactionAmount out of known range\n",
    "    _, bins = pd.qcut(all_data.transactionAmount, n, retbins=True)\n",
    "    bins[0] = -np.inf\n",
    "    bins[-1] = np.inf\n",
    "    \n",
    "    data['transactionAmount'] = pd.cut(data.transactionAmount, bins)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def bin_cash(data):\n",
    "    \"\"\"\n",
    "    To cut avalibleCash into bins\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: data frame, transaction data\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    data frame\n",
    "    \"\"\"\n",
    "    # the bins are made manually with the help of histogram\n",
    "    bins = [-np.inf, 1500, 2500, 4500, 7500, 8500, 10500, np.inf]\n",
    "    data['availableCash'] = pd.cut(data.availableCash, bins)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and cleaning data ..........................................\n",
      "\n",
      "Preprocessing train and test data ..................................\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading and cleaning data ..........................................\\n\")\n",
    "# fraud labeled data\n",
    "frauded = pd.read_csv(\"labels_obf.csv\")\n",
    "frauded[\"fraud\"] = 1\n",
    "frauded.drop(\"reportedTime\", axis=1, inplace=True)\n",
    "\n",
    "# transaction data\n",
    "trans = pd.read_csv(\"transactions_obf.csv\", \n",
    "                    parse_dates=['transactionTime'],\n",
    "                    index_col=0)\n",
    "original_trans = trans.copy()\n",
    "trans = clean_trans(trans)\n",
    "\n",
    "\n",
    "# labeled transaction data\n",
    "lab_trans = pd.merge(trans, frauded, on='eventId', how='left')\n",
    "lab_trans.drop([\"eventId\"], axis=1, inplace=True)\n",
    "lab_trans.fraud.fillna(0, inplace=True)\n",
    "lab_trans.fraud = lab_trans.fraud.astype(int)\n",
    "\n",
    "features = ['accountNumber', 'merchantId', 'mcc', 'merchantCountry',\n",
    "           'merchantZip', 'posEntryMode']\n",
    "lab_trans = keep_levels(lab_trans, features)\n",
    "\n",
    "\n",
    "#%% prepare train and test data\n",
    "print(\"Preprocessing train and test data ..................................\\n\")\n",
    "\n",
    "train = lab_trans[trans.index.year == 2017]\n",
    "test = lab_trans[trans.index.year == 2018]\n",
    "\n",
    "y_train = train.fraud\n",
    "y_test = test.fraud\n",
    "X_train = train.drop(['fraud'], axis=1)\n",
    "X_test = test.drop(['fraud'], axis=1)\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "        transformers=[('cats', OneHotEncoder(drop='first'), list(range(10)))])\n",
    "col_trans.fit(pd.concat([X_train, X_test], axis=0))\n",
    "X_train = col_trans.transform(X_train)\n",
    "X_test = col_trans.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the logistic regression model .............................\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the logistic regression model .............................\\n\")\n",
    "\n",
    "# grid search we set the class_weight to the class ratio and tune a couple of C\n",
    "model_params = {\"C\": [0.1, 1, 10]}\n",
    "\n",
    "logit = LogisticRegression(random_state=9876, \n",
    "                           class_weight={0:1, 1:135},\n",
    "                           max_iter=300)\n",
    "\n",
    "grid = GridSearchCV(logit, model_params, scoring=\"roc_auc\", cv=10, \n",
    "                    n_jobs=-1, verbose=0)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Evaluate Final model on test data ..................................\\n\")\n",
    "y_prob = grid.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "print(f\"The final model has a AUC score of {auc} on test data.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating loss prevention with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating fraud prevention of the model ..........................\\n\")\n",
    "\n",
    "# Find the top 400 transaction for human team to review and evaluation how much\n",
    "# loss prevented by the model\n",
    "orig_test = original_trans[original_trans.index.year == 2018]\n",
    "orig_test.reset_index(inplace=True)\n",
    "orig_test['fraud'] = list(test.fraud)\n",
    "orig_test['prob'] = y_prob[:, 1]\n",
    "orig_test['possible_loss'] = orig_test.transactionAmount * orig_test.prob\n",
    "orig_test.sort_values('possible_loss', ascending=False, inplace=True)\n",
    "\n",
    "top_400 = orig_test.iloc[0:400, :]\n",
    "\n",
    "top_eventId = list(top_400.eventId)\n",
    "print(f\"Here are the eventID of 400 transactions to review:\\n{top_eventId}\\n\")\n",
    "\n",
    "# fraud summary in 2018 Jan\n",
    "fraud_times = sum(test.fraud)\n",
    "total_loss = round(sum(orig_test.transactionAmount * orig_test.fraud), 2)\n",
    "print(f\"There are {fraud_times} frauds reported in 2018 January.\")\n",
    "print(f\"The total loss is {total_loss} GBP.\\n\")\n",
    "\n",
    "# fraud and loss identified by model\n",
    "identified_fraud = sum(top_400.fraud)\n",
    "prevent_loss = round(sum(top_400.transactionAmount * top_400.fraud), 2)\n",
    "print(f\"The model identified {identified_fraud} frauds.\")\n",
    "print(f\"The total prevented loss is {prevent_loss} GBP.\\n\")\n",
    "\n",
    "pct = round((prevent_loss / total_loss) * 100, 2)\n",
    "print(f\"We prevented {pct}% fraud loss.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
