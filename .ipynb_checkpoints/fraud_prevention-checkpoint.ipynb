{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks and methods:\n",
    "- We will identify ~400 high risk transactions each month for the team to\n",
    "  review.\n",
    "- The model will predict the probability of a transaction being fraud. \n",
    "  - Use logistic regression as it produces well-defined probability.\n",
    "  - Use 2017 data as training data and 2018 Jan data as test data\n",
    "- Metrics: as much fraud loss prevented as possible. \n",
    "  - For this purpose we will use fraud_probability * transactionAmount to rank\n",
    "    each transaction.\n",
    "  - Pick the top 400 transaction for review.\n",
    "\n",
    "## Retults:\n",
    "- We set January 2018 as the test data, which has 39 frauds reported with a\n",
    "  total loss of 5814.46 GBP.\n",
    "- Using the model and team review, we identified 10 frauds and prevented loss\n",
    "  of 4079.30 GBP.\n",
    "- We prevented 70% fraud loss in terms of value.\n",
    "\n",
    "## Model training\n",
    "### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.metrics import recall_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_levels(data, features, lab_data=None, n=20):\n",
    "    \"\"\"\n",
    "    For categorical features, keep levels that have at least n fraud reported\n",
    "    and all other levels groupd as \"others\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: data frame whose categorical features are processed\n",
    "    features: string, list of selected features to be process\n",
    "    lab_data: labeled transaction data to generate fraud count of each levels\n",
    "    n: integer, threshold of reported fraud cases. \n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    data frame\n",
    "    \"\"\"\n",
    "    if lab_data is None:\n",
    "        lab_data = data\n",
    "    \n",
    "    for ft in features:\n",
    "        total_fraud = lab_data.groupby(ft).fraud.sum()\n",
    "        level_keep = total_fraud.index[total_fraud >= n].tolist()\n",
    "        data.loc[~data[ft].isin(level_keep), ft] = 'others'\n",
    "        \n",
    "    return data\n",
    "\n",
    "def clean_trans(data):\n",
    "    \"\"\"\n",
    "    To clean transaction data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: data frame, in which 'transactionTime' was set as index\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    data frame\n",
    "    \"\"\"\n",
    "    data = change_types(data)\n",
    "    data = bin_trans_amount(data)\n",
    "    data = bin_cash(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def change_types(data):\n",
    "    \"\"\"\n",
    "    To change data types of features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: data frame, in which 'transactionTime' was set as index\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    data frame\n",
    "    \"\"\"\n",
    "    \n",
    "    trans['month'] = trans.index.month.astype(str)\n",
    "    trans['weekday'] = trans.index.weekday.astype(str)\n",
    "    trans['hour'] = trans.index.hour.astype(str)\n",
    "\n",
    "    # convert integer to string\n",
    "    for ft in [\"mcc\", \"merchantCountry\", \"posEntryMode\"]:\n",
    "        trans[ft] = trans[ft].astype(str)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def bin_trans_amount(data, all_data=None, n=10):\n",
    "    \"\"\"\n",
    "    To cut 'transactionAmount' into n bins with roughly equal size.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: data frame, transaction data in which transactionAmount is binned\n",
    "    all_data: data frame, all transactions used to find bins\n",
    "    n: integer, number of bins\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    data frame of binned data\n",
    "    \"\"\"\n",
    "    if all_data is None:\n",
    "        all_data = data.copy()\n",
    "    \n",
    "    # use pd.qcut to first find the bins with equal number of samples and then\n",
    "    # set ends to infinity to allow new transactionAmount out of known range\n",
    "    _, bins = pd.qcut(all_data.transactionAmount, n, retbins=True)\n",
    "    bins[0] = -np.inf\n",
    "    bins[-1] = np.inf\n",
    "    \n",
    "    data['transactionAmount'] = pd.cut(data.transactionAmount, bins)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def bin_cash(data):\n",
    "    \"\"\"\n",
    "    To cut avalibleCash into bins\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: data frame, transaction data\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    data frame\n",
    "    \"\"\"\n",
    "    # the bins are made manually with the help of histogram\n",
    "    bins = [-np.inf, 1500, 2500, 4500, 7500, 8500, 10500, np.inf]\n",
    "    data['availableCash'] = pd.cut(data.availableCash, bins)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and cleaning data ..........................................\n",
      "\n",
      "Preprocessing train and test data ..................................\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading and cleaning data ..........................................\\n\")\n",
    "# fraud labeled data\n",
    "frauded = pd.read_csv(\"labels_obf.csv\")\n",
    "frauded[\"fraud\"] = 1\n",
    "frauded.drop(\"reportedTime\", axis=1, inplace=True)\n",
    "\n",
    "# transaction data\n",
    "trans = pd.read_csv(\"transactions_obf.csv\", \n",
    "                    parse_dates=['transactionTime'],\n",
    "                    index_col=0)\n",
    "original_trans = trans.copy()\n",
    "trans = clean_trans(trans)\n",
    "\n",
    "\n",
    "# labeled transaction data\n",
    "lab_trans = pd.merge(trans, frauded, on='eventId', how='left')\n",
    "lab_trans.drop([\"eventId\"], axis=1, inplace=True)\n",
    "lab_trans.fraud.fillna(0, inplace=True)\n",
    "lab_trans.fraud = lab_trans.fraud.astype(int)\n",
    "\n",
    "features = ['accountNumber', 'merchantId', 'mcc', 'merchantCountry',\n",
    "           'merchantZip', 'posEntryMode']\n",
    "lab_trans = keep_levels(lab_trans, features)\n",
    "\n",
    "\n",
    "#%% prepare train and test data\n",
    "print(\"Preprocessing train and test data ..................................\\n\")\n",
    "\n",
    "train = lab_trans[trans.index.year == 2017]\n",
    "test = lab_trans[trans.index.year == 2018]\n",
    "\n",
    "y_train = train.fraud\n",
    "y_test = test.fraud\n",
    "X_train = train.drop(['fraud'], axis=1)\n",
    "X_test = test.drop(['fraud'], axis=1)\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "        transformers=[('cats', OneHotEncoder(drop='first'), list(range(10)))])\n",
    "col_trans.fit(pd.concat([X_train, X_test], axis=0))\n",
    "X_train = col_trans.transform(X_train)\n",
    "X_test = col_trans.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the logistic regression model .............................\n",
      "\n",
      "Evaluate Final model on test data ..................................\n",
      "\n",
      "The final model has a AUC score of 0.8898420232617125 on test data.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gl/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the logistic regression model .............................\\n\")\n",
    "\n",
    "# grid search we set the class_weight to the class ratio and tune a couple of C\n",
    "model_params = {\"C\": [0.1, 1, 10]}\n",
    "\n",
    "logit = LogisticRegression(random_state=9876, \n",
    "                           class_weight={0:1, 1:135},\n",
    "                           max_iter=300)\n",
    "\n",
    "grid = GridSearchCV(logit, model_params, scoring=\"roc_auc\", cv=10, \n",
    "                    n_jobs=-1, verbose=0)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Evaluate Final model on test data ..................................\\n\")\n",
    "y_prob = grid.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "print(f\"The final model has a AUC score of {auc} on test data.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating loss prevention with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fraud prevention of the model ..........................\n",
      "\n",
      "Here are the eventID of 400 transactions to review:\n",
      "['104978867A1', '38334020A1', '235076A1', '38333300A1', '101582131A1', '66602299A1', '229652A1', '89090664A1', '47558819A1', '18913058A1', '67544354A1', '10924054A1', '2238159A1', '4660175A1', '4642991A1', '56168163A1', '8287018A1', '47545139A1', '91562949A1', '89121864A1', '52723994A1', '10924870A1', '103327552A1', '38020207A1', '45501371A1', '94175977A1', '65649276A1', '4666463A1', '38206066A1', '38227762A1', '38207074A1', '66592027A1', '85400938A1', '81470726A1', '65652492A1', '93566706A1', '58132138A1', '89824057A1', '48278423A1', '38336612A1', '10930438A1', '89104872A1', '48278087A1', '10923910A1', '54138392A1', '36067171A1', '21905089A1', '10930582A1', '16620608A1', '84968647A1', '39566285A1', '18052038A1', '104982563A1', '77953603A1', '4664687A1', '10923862A1', '28448454A1', '72197551A1', '10925782A1', '4906191A1', '103275136A1', '66579739A1', '94328473A1', '78123185A1', '101043067A1', '64096442A1', '106375901A1', '106375621A1', '87383241A1', '87381945A1', '2240799A1', '39566573A1', '12520639A1', '38341940A1', '66599995A1', '2233599A1', '60172697A1', '60171881A1', '22409057A1', '101068836A1', '77648723A1', '1212134A1', '84977863A1', '56159571A1', '18677583A1', '22406033A1', '16612736A1', '85820094A1', '3514744A1', '4609668A1', '102768568A1', '29496448A1', '76111390A1', '22611521A1', '103190434A1', '12782557A1', '14587862A1', '68546633A1', '101135074A1', '13225500A1', '66601387A1', '84969847A1', '106940968A1', '2736898A1', '34080496A1', '87385257A1', '80390483A1', '68548217A1', '106276463A1', '4055999A1', '92298215A1', '66588235A1', '39557453A1', '19523365A1', '101054724A1', '66593035A1', '40675665A1', '64168586A1', '64340664A1', '85813950A1', '28419090A1', '62681276A1', '23792468A1', '20320299A1', '62278757A1', '3514792A1', '15926483A1', '55462625A1', '8230977A1', '106941092A1', '56167107A1', '61294217A1', '30638320A1', '13226028A1', '48585232A1', '13224204A1', '62282885A1', '5152885A1', '47249674A1', '77658083A1', '28076644A1', '81470582A1', '39565613A1', '101581651A1', '64452931A1', '69799383A1', '94157794A1', '57261668A1', '1183190A1', '47227306A1', '77652851A1', '75604631A1', '47237482A1', '47563811A1', '71058467A1', '87389241A1', '28451574A1', '4051391A1', '34297140A1', '12077358A1', '3514840A1', '106940658A1', '103321984A1', '67544546A1', '42523452A1', '38218498A1', '12785149A1', '54423303A1', '73063598A1', '66838579A1', '85808766A1', '21052366A1', '34910790A1', '1219094A1', '39565805A1', '28413762A1', '78482904A1', '20580588A1', '78126689A1', '39256169A1', '28418706A1', '90925502A1', '4652879A1', '28075204A1', '22388129A1', '28450422A1', '28886241A1', '39611968A1', '102806694A1', '77652947A1', '10929862A1', '22826792A1', '47250154A1', '4654463A1', '103291840A1', '76503003A1', '87748932A1', '16611824A1', '28449558A1', '84604446A1', '3514696A1', '9980296A1', '54057663A1', '102817782A1', '33971505A1', '95642729A1', '85825758A1', '1177862A1', '38226322A1', '31214773A1', '13027286A1', '31296401A1', '87723588A1', '98627566A1', '39254057A1', '19196542A1', '54033951A1', '64336680A1', '8897306A1', '8884634A1', '8894618A1', '8886938A1', '85812990A1', '28066324A1', '42459130A1', '19523221A1', '95636777A1', '79966272A1', '88475931A1', '105513825A1', '38233186A1', '38209618A1', '46534885A1', '38218738A1', '38228098A1', '38234242A1', '95953894A1', '46824282A1', '88496523A1', '14215297A1', '3514648A1', '28403202A1', '42851313A1', '72846556A1', '72832060A1', '93393257A1', '30856447A1', '38019007A1', '28447062A1', '26664925A1', '54071522A1', '14672884A1', '13918054A1', '77962579A1', '58145002A1', '76111630A1', '23794292A1', '28072468A1', '1206230A1', '39557357A1', '106444673A1', '66575947A1', '23792420A1', '23790308A1', '10721648A1', '56171139A1', '98628958A1', '39604000A1', '45491219A1', '56159523A1', '51260211A1', '102782056A1', '46989440A1', '76502907A1', '72854436A1', '54071138A1', '31143268A1', '24734381A1', '78490536A1', '18127479A1', '20738396A1', '9950584A1', '28073380A1', '28049716A1', '28061044A1', '66607579A1', '16612304A1', '28064308A1', '84972631A1', '85370771A1', '54065186A1', '104980547A1', '28052596A1', '66278138A1', '21389770A1', '21389866A1', '62289173A1', '15927635A1', '105472136A1', '105465128A1', '27646419A1', '20127422A1', '62284565A1', '23793716A1', '41809132A1', '36072360A1', '91738558A1', '91839036A1', '63141093A1', '106943138A1', '15928787A1', '15929987A1', '56163795A1', '66594763A1', '37718893A1', '37715965A1', '51272739A1', '51268995A1', '47247898A1', '93565986A1', '95956486A1', '47235514A1', '66602107A1', '37713037A1', '95641241A1', '22827128A1', '93442361A1', '44446684A1', '105461528A1', '86780295A1', '65424187A1', '65421739A1', '105473192A1', '21306873A1', '20733548A1', '46986944A1', '89125272A1', '62282261A1', '72675933A1', '55453169A1', '102754840A1', '20323659A1', '28080340A1', '5696277A1', '69698261A1', '87393321A1', '106780344A1', '3000061A1', '101066820A1', '21058318A1', '30632848A1', '58246885A1', '19524901A1', '84048607A1', '1632204A1', '27220120A1', '54137912A1', '73063742A1', '33975441A1', '76502331A1', '39255113A1', '39253481A1', '645822A1', '234932A1', '47249194A1', '1193654A1', '68545097A1', '69084231A1', '29039793A1', '90122082A1', '79183197A1', '60880591A1', '2736802A1', '102774856A1', '38225698A1', '64340472A1', '40665777A1', '12788893A1', '89101416A1', '13932406A1', '72848148A1', '48693018A1', '65419195A1', '47223610A1', '4666895A1', '31214053A1', '17781519A1', '89091000A1', '11679354A1', '58922517A1', '79824399A1', '69812295A1', '77970403A1', '62291765A1', '30134965A1', '84951415A1', '46143918A1', '46141758A1', '46143870A1', '46139886A1']\n",
      "\n",
      "There are 39 frauds reported in 2018 January.\n",
      "The total loss is 5814.46 GBP.\n",
      "\n",
      "The model identified 10 frauds.\n",
      "The total prevented loss is 4079.3 GBP.\n",
      "\n",
      "We prevented 70.16% fraud loss.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating fraud prevention of the model ..........................\\n\")\n",
    "\n",
    "# Find the top 400 transaction for human team to review and evaluation how much\n",
    "# loss prevented by the model\n",
    "orig_test = original_trans[original_trans.index.year == 2018]\n",
    "orig_test.reset_index(inplace=True)\n",
    "orig_test['fraud'] = list(test.fraud)\n",
    "orig_test['prob'] = y_prob[:, 1]\n",
    "orig_test['possible_loss'] = orig_test.transactionAmount * orig_test.prob\n",
    "orig_test.sort_values('possible_loss', ascending=False, inplace=True)\n",
    "\n",
    "top_400 = orig_test.iloc[0:400, :]\n",
    "\n",
    "top_eventId = list(top_400.eventId)\n",
    "print(f\"Here are the eventID of 400 transactions to review:\\n{top_eventId}\\n\")\n",
    "\n",
    "# fraud summary in 2018 Jan\n",
    "fraud_times = sum(test.fraud)\n",
    "total_loss = round(sum(orig_test.transactionAmount * orig_test.fraud), 2)\n",
    "print(f\"There are {fraud_times} frauds reported in 2018 January.\")\n",
    "print(f\"The total loss is {total_loss} GBP.\\n\")\n",
    "\n",
    "# fraud and loss identified by model\n",
    "identified_fraud = sum(top_400.fraud)\n",
    "prevent_loss = round(sum(top_400.transactionAmount * top_400.fraud), 2)\n",
    "print(f\"The model identified {identified_fraud} frauds.\")\n",
    "print(f\"The total prevented loss is {prevent_loss} GBP.\\n\")\n",
    "\n",
    "pct = round((prevent_loss / total_loss) * 100, 2)\n",
    "print(f\"We prevented {pct}% fraud loss.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
